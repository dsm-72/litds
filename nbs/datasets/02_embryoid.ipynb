{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embryoid Bodies 2018\n",
    "\n",
    "> Embryoid Bodies 2018 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.embryoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, math\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "import torch, torch.nn as nn, pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt, seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "\n",
    "from beartype.typing import Optional, Tuple, Union, TypeAlias, Dict, Any\n",
    "from nptyping import NDArray, Float, Shape, Number as AnyNumber\n",
    "from beartype import beartype\n",
    "\n",
    "#| export\n",
    "from littyping.core import Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.utils import wrangle_kwargs_for_func, make_missing_dirs\n",
    "from iza.static import SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from litds.abc.dfdm.base import set_dataset\n",
    "\n",
    "from litds.core.dfds import (DataFrameDataset)\n",
    "from litds.core.dfdm import (DataFrameDataModule)\n",
    "from litds.time.base import (TimeDataset, TimeDataModule)\n",
    "\n",
    "from litds.utils import (random_split_dataframe)\n",
    "\n",
    "from litds.types import (\n",
    "    Number, DataFrame, IndexLike, SeriesLike, GroupKey,  \n",
    "    TrainValidTestSplit,\n",
    "    XYArray, DistanceArray, LabelArray,    \n",
    "    SequenceWithLength, SequencesWithLengths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from kuut.core import (kuut, kstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests, zipfile, io\n",
    "import phate, scprep, pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MENDELY_URI = (\n",
    "    'https://data.mendeley.com/public-files/'\n",
    "    'datasets/v6n743h5ng/files/b1865840-e8df-4381-'\n",
    "    '8866-b04d57309e1d/file_downloaded'\n",
    ")\n",
    "\n",
    "SINGLE_CELL_SUBDIR = 'scRNAseq'\n",
    "# SAMPLES = 'samples'\n",
    "\n",
    "TIMEPOINT_DIRS = 'T0_1A T2_3B T4_5C T6_7D T8_9E'.split()\n",
    "\n",
    "HUMAN_FRIENDLY_TIMEPOINTS = 'Day 00-03,Day 06-09,Day 12-15,Day 18-21,Day 24-27'.split(',')\n",
    "\n",
    "STEPS = [\n",
    "    kstep(1, val=20, desc='download'),\n",
    "    kstep(2, val=5, sub=0, desc='load_timepoints'),\n",
    "    kstep(2, val=5, sub=1, desc='load_timepoints'),\n",
    "    kstep(2, val=5, sub=2, desc='load_timepoints'),\n",
    "    kstep(2, val=5, sub=3, desc='load_timepoints'),\n",
    "    kstep(2, val=5, sub=4, desc='load_timepoints'),\n",
    "\n",
    "    kstep(3, val=5, sub=0, desc='library_fitlering'),\n",
    "    kstep(3, val=5, sub=1, desc='library_fitlering'),\n",
    "    kstep(3, val=5, sub=2, desc='library_fitlering'),\n",
    "    kstep(3, val=5, sub=3, desc='library_fitlering'),\n",
    "    kstep(3, val=5, sub=4, desc='library_fitlering'),\n",
    "    \n",
    "    kstep(4, val=5, desc='merge_data'),\n",
    "    kstep(5, val=5, sub=0, desc='filter_data'),\n",
    "    kstep(5, val=5, sub=1, desc='filter_data'),\n",
    "    kstep(5, val=5, sub=2, desc='filter_data'),\n",
    "    kstep(5, val=5, sub=3, desc='filter_data'),\n",
    "    kstep(5, val=5, sub=4, desc='filter_data'),\n",
    "    kstep(5, val=5, sub=5, desc='filter_data'),\n",
    "    \n",
    "    kstep(6, val=20, desc='embed'),\n",
    "    kstep(7, desc='save'),\n",
    "]\n",
    "\n",
    "REQUIRED_FILES = [\n",
    "    'df_counts.pkl', 'df_phate.pkl', \n",
    "    'df_pca.pkl', 'phate_op.pkl'\n",
    "]\n",
    "\n",
    "REQUIRED_ATTRS = [\n",
    "    'df_counts', 'df_phate', 'df_pca',\n",
    "    'phate_op', 'pca_loadings', 'pca_components'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class EmbryoidBodies2018DataModule(pl.LightningDataModule):\n",
    "    time_col: str = SAMPLES\n",
    "    data_dir: str = os.path.expanduser('~/Downloads/embryoid_2018')\n",
    "    batch_size: int = 32\n",
    "    use_time_dataset:bool = True\n",
    "\n",
    "    primary: str = 'counts'\n",
    "    dl_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    _: KW_ONLY\n",
    "    perc_train: float = 0.7\n",
    "    perc_valid: float = 0.1\n",
    "    perc_test: float = 0.2\n",
    "\n",
    "    def get_primary_df(self):\n",
    "        match self.primary:\n",
    "            case 'counts':\n",
    "                return self.df_counts\n",
    "            case 'phate':\n",
    "                return self.df_phate\n",
    "            case 'pca':\n",
    "                return self.df_pca\n",
    "            case _:\n",
    "                raise ValueError\n",
    "\n",
    "    @property\n",
    "    def pbar(self):\n",
    "        try:\n",
    "            return self._pbar\n",
    "        except AttributeError:\n",
    "            self._pbar = kuut(steps=STEPS, desc='EB Loader')\n",
    "            self._pbar.display()\n",
    "            return self._pbar\n",
    "    \n",
    "    @property\n",
    "    def sc_rna_dir(self):\n",
    "        return os.path.join(self.data_dir, SINGLE_CELL_SUBDIR)\n",
    "    \n",
    "    @property\n",
    "    def timepoint_dirs(self):\n",
    "        return [os.path.join(self.sc_rna_dir, d) for d in TIMEPOINT_DIRS]\n",
    "    \n",
    "    @property\n",
    "    def has_all_timepoint_dirs(self):\n",
    "        return all(os.path.exists(d) for d in self.timepoint_dirs)\n",
    "    \n",
    "    @property\n",
    "    def does_data_dir_exists(self):\n",
    "        return os.path.isdir(self.data_dir)\n",
    "\n",
    "    @property\n",
    "    def does_sc_rna_dir_exists(self):\n",
    "        return os.path.isdir(self.sc_rna_dir)\n",
    "    \n",
    "    @property\n",
    "    def has_all_saved_files(self):\n",
    "        try:\n",
    "            files = os.listdir(self.data_dir)\n",
    "            return all(list(map(lambda e: e in files, REQUIRED_FILES)))  \n",
    "              \n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        \n",
    "    @property\n",
    "    def is_data_downloaded(self):\n",
    "        return self.does_sc_rna_dir_exists and self.has_all_saved_files\n",
    "        \n",
    "    def make_data_dir(self):        \n",
    "        make_missing_dirs([self.data_dir])\n",
    "\n",
    "    def make_sc_rna_dir(self):        \n",
    "        make_missing_dirs([self.sc_rna_dir])\n",
    "\n",
    "    \n",
    "    def download(self):\n",
    "        self.pbar.step(0)\n",
    "        if self.has_all_saved_files:\n",
    "            pass\n",
    "        \n",
    "        if not self.has_all_timepoint_dirs:\n",
    "            self.make_sc_rna_dir()\n",
    "            self.pbar.write(f'Could not find eb data at {self.data_dir}. Downloading...')\n",
    "            r = requests.get(MENDELY_URI)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(self.data_dir)\n",
    "\n",
    "        self.pbar.step(1)\n",
    "\n",
    "    def _load_timepoint(self, t:str, sparse:bool=True):\n",
    "        return scprep.io.load_10X(os.path.join(self.sc_rna_dir, t), sparse=sparse, gene_labels='both')\n",
    "    \n",
    "    def load_timepoints(self, sparse:bool=True):\n",
    "        self.raw = {}\n",
    "        self.pbar.write(f'Loading 10X genomics data')\n",
    "        for i, t in enumerate(TIMEPOINT_DIRS):            \n",
    "            self.raw[i] = self._load_timepoint(t, sparse)\n",
    "            self.pbar.step(2, i)\n",
    "            \n",
    "        self.pbar.step(2)\n",
    "        return self.raw\n",
    "    \n",
    "    def library_fitlering(self):\n",
    "        self.filtered = []\n",
    "        self.pbar.write(f'Filtering library size')\n",
    "        for i, (t, batch) in enumerate(self.raw.items()):\n",
    "            batch = scprep.filter.filter_library_size(batch, percentile=20, keep_cells='above')\n",
    "            batch = scprep.filter.filter_library_size(batch, percentile=75, keep_cells='below')\n",
    "            self.filtered.append(batch)\n",
    "            self.pbar.step(3, i)\n",
    "            \n",
    "        del self.raw\n",
    "        self.raw = None\n",
    "        self.pbar.step(3)\n",
    "        return self.filtered\n",
    "    \n",
    "    def merge_data(self):\n",
    "        self.df_counts, self.sample_labels = scprep.utils.combine_batches(\n",
    "            self.filtered, HUMAN_FRIENDLY_TIMEPOINTS, append_to_cell_names=True\n",
    "        )\n",
    "        del self.filtered\n",
    "        self.pbar.step(4)        \n",
    "        return self.df_counts\n",
    "    \n",
    "    def filter_data(self):\n",
    "        self.pbar.write('Filtering genes and normalizing')\n",
    "        self.df_counts = scprep.filter.filter_rare_genes(self.df_counts, min_cells=10)\n",
    "        self.pbar.step(5, 0)        \n",
    "\n",
    "        self.df_counts = scprep.normalize.library_size_normalize(self.df_counts)\n",
    "        self.pbar.step(5, 1)        \n",
    "\n",
    "        # Get all mitochondrial genes. There are 14, FYI.\n",
    "        mito_genes = scprep.select.get_gene_set(self.df_counts, starts_with=\"MT-\")\n",
    "        self.pbar.step(5, 2)        \n",
    "\n",
    "        self.df_counts, self.sample_labels = scprep.filter.filter_gene_set_expression(\n",
    "            self.df_counts, self.sample_labels, genes=mito_genes, \n",
    "            percentile=90, keep_cells='below'\n",
    "        )\n",
    "        self.pbar.step(5, 3)\n",
    "        \n",
    "        self.df_counts = scprep.transform.sqrt(self.df_counts)\n",
    "        self.pbar.step(5, 4)\n",
    "        \n",
    "        self.df_counts = self.df_counts.join(self.sample_labels)\n",
    "        self.df_counts = self.df_counts.rename({'sample_labels': self.time_col}, axis=1)        \n",
    "        self.df_counts.columns = self.df_counts.columns.map(lambda e: e.split(' ')[0])\n",
    "        self.pbar.step(5, 5)\n",
    "        \n",
    "        self.pbar.step(5)\n",
    "\n",
    "    def embed(self):\n",
    "        self.pbar.write('Embedding data')\n",
    "        self.phate_op = phate.PHATE(n_jobs=-2, random_state=42)\n",
    "        self.Y_phate = self.phate_op.fit_transform(\n",
    "            self.df_counts.drop(columns=self.time_col)\n",
    "        )\n",
    "\n",
    "        self.df_phate = pd.DataFrame(\n",
    "            np.hstack((\n",
    "                self.Y_phate, \n",
    "                self.sample_labels.values.reshape(-1, 1)\n",
    "            )), index=self.df_counts.index, columns=f'P1 P2 {self.time_col}'.split()\n",
    "        )\n",
    "        \n",
    "        self.pca_loadings = self.phate_op.graph.data_nu\n",
    "        self.pca_components = self.phate_op.graph.data_pca.components_\n",
    "        self.df_pca = pd.DataFrame(\n",
    "            self.pca_loadings, \n",
    "            columns=[f'd{i}' for i in range(1, 101)],\n",
    "        )\n",
    "        self.df_pca.loc[:, self.time_col] = pd.Series(\n",
    "            self.sample_labels.values, name=self.time_col\n",
    "        )\n",
    "        \n",
    "        self.pbar.step(6)\n",
    "        \n",
    "        return self.df_phate\n",
    "    \n",
    "    def save(self):\n",
    "        self.pbar.write('Saving')\n",
    "        self.make_data_dir()\n",
    "        self.df_counts.to_pickle(os.path.join(self.data_dir, 'df_counts.pkl'))\n",
    "        self.df_phate.to_pickle(os.path.join(self.data_dir, 'df_phate.pkl'))\n",
    "        self.df_pca.to_pickle(os.path.join(self.data_dir, 'df_pca.pkl'))\n",
    "        with open(os.path.join(self.data_dir, 'phate_op.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.phate_op, f)\n",
    "            \n",
    "        self.pbar.step(7)\n",
    "\n",
    "\n",
    "    def pipeline(self):\n",
    "        self.load_timepoints()\n",
    "        self.library_fitlering()\n",
    "        self.merge_data()\n",
    "        self.filter_data()\n",
    "        self.embed()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        '''\n",
    "        Notes\n",
    "        -----\n",
    "        - prepare_data is called from the main process. It is not recommended \n",
    "            to assign state here (e.g. self.x = y) since it is called on a \n",
    "            single process and if you assign states here then they wonâ€™t be \n",
    "            available for other processes.\n",
    "        '''\n",
    "        # download\n",
    "        self.download()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def has_all_attrs(self):        \n",
    "        return all(list(map(lambda a: hasattr(self, a), REQUIRED_ATTRS)))\n",
    "\n",
    "\n",
    "    def load_or_download(self):\n",
    "        if self.has_all_saved_files:\n",
    "            self.load()\n",
    "        else:\n",
    "            self.download()\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        self.pbar.write('Loading data')\n",
    "        \n",
    "        if getattr(self, 'df_counts', None) is None:\n",
    "            self.df_counts = pd.read_pickle(os.path.join(self.data_dir, 'df_counts.pkl'))\n",
    "            \n",
    "        if getattr(self, 'df_cdf_phateounts', None) is None:\n",
    "            self.df_phate = pd.read_pickle(os.path.join(self.data_dir, 'df_phate.pkl'))\n",
    "            \n",
    "        if getattr(self, 'df_pca', None) is None:\n",
    "            self.df_pca = pd.read_pickle(os.path.join(self.data_dir, 'df_pca.pkl'))\n",
    "\n",
    "        if getattr(self, 'phate_op', None) is None:\n",
    "            with open(os.path.join(self.data_dir, 'phate_op.pkl'), 'rb') as f:\n",
    "                self.phate_op = pickle.load(f)    \n",
    "                \n",
    "        if getattr(self, 'pca_loadings', None) is None:\n",
    "            self.pca_loadings = self.phate_op.graph.data_nu\n",
    "            \n",
    "        if getattr(self, 'pca_components', None) is None:\n",
    "            self.pca_components = self.phate_op.graph.data_pca.components_\n",
    "            \n",
    "        self.pbar.step(7)\n",
    "        self.pbar.write('Data ready!')\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str]=None):\n",
    "        # load\n",
    "        self.load_or_download()\n",
    "        \n",
    "        # filtering\n",
    "        if not self.has_all_attrs:\n",
    "            self.pipeline()\n",
    "            self.save()\n",
    "            \n",
    "        idxs_train, idxs_valid, idxs_test = random_split_dataframe(\n",
    "            self.get_primary_df().drop(columns=self.time_col),\n",
    "            [self.perc_train, self.perc_valid, self.perc_test],\n",
    "            as_dataframes=False\n",
    "        )\n",
    "        self.idxs_train = idxs_train\n",
    "        self.idxs_valid = idxs_valid\n",
    "        self.idxs_test = idxs_test\n",
    "            \n",
    "        if stage == 'fit':\n",
    "            pass\n",
    "        \n",
    "        if stage == 'test':\n",
    "            pass\n",
    "        \n",
    "        if stage == 'predict':\n",
    "            pass\n",
    "\n",
    "\n",
    "    def make_dataset(self, df: DataFrame):\n",
    "        if self.use_time_dataset:\n",
    "            ds = TimeDataset(df, time_key=self.time_col)        \n",
    "        else:\n",
    "            ds = DataFrameDataset(df, label_col=self.time_col)\n",
    "        return ds\n",
    "    \n",
    "    def make_dataloader(self, ds):\n",
    "        # NOTE: makes sure batch_size overwrites values in dl_kwargs\n",
    "        kwargs = dict(**self.dl_kwargs, **dict(batch_size=self.batch_size, drop_last=True))\n",
    "        dl = DataLoader(ds, **kwargs)\n",
    "        return dl\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        df_sub = self.get_primary_df().iloc[self.idxs_train]\n",
    "        ds = self.make_dataset(df_sub)\n",
    "        self.train_ds = ds\n",
    "        return self.make_dataloader(ds)\n",
    "        \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        df_sub = self.get_primary_df().iloc[self.idxs_valid]\n",
    "        ds = self.make_dataset(df_sub)\n",
    "        self.valid_ds = ds\n",
    "        return self.make_dataloader(ds)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        df_sub = self.get_primary_df().iloc[self.idxs_test]\n",
    "        ds = self.make_dataset(df_sub)\n",
    "        self.test_ds = ds\n",
    "        return self.make_dataloader(ds)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        '''\n",
    "        See Also\n",
    "        --------\n",
    "        https://github.com/Lightning-AI/lightning/discussions/11455#discussioncomment-1959264\n",
    "        \n",
    "        Notes\n",
    "        -----        \n",
    "        Test: is used for a holdout section of your dataset, used to evaluate a model after \n",
    "            you've done hyperparameter tuning for fair comparison. The test set has labels \n",
    "            included\n",
    "            \n",
    "        Prediction: There are no labels available, only the model inputs, meaning prediction isn't \n",
    "            used for evaluation\n",
    "\n",
    "        Regarding differences in Lightning: the two code paths are pretty similar are very similar. \n",
    "            The model is put into eval mode, gradients are disabled, and the trainer makes one \n",
    "            pass through the corresponding dataloader(s), calling the relevant hooks in the \n",
    "            Lightning Module or callback (prefixed with test or predict)\n",
    "        '''\n",
    "        return self.test_dataloader()\n",
    "\n",
    "    def teardown(self, stage:str):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: False\n",
    "eb = EmbryoidBodies2018DataModule(\n",
    "    primary='pca',\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: False\n",
    "eb.does_data_dir_exists, eb.does_sc_rna_dir_exists, eb.has_all_timepoint_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e9f3d387fd4270996d7731f9b19dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EB Loader:   0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Data ready!\n"
     ]
    }
   ],
   "source": [
    "#| eval: False\n",
    "eb.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: False\n",
    "dl = eb.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d97</th>\n",
       "      <th>d98</th>\n",
       "      <th>d99</th>\n",
       "      <th>d100</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>0.881895</td>\n",
       "      <td>-1.463558</td>\n",
       "      <td>0.309755</td>\n",
       "      <td>-0.156539</td>\n",
       "      <td>Day 06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15125</th>\n",
       "      <td>1.099650</td>\n",
       "      <td>-1.794608</td>\n",
       "      <td>-1.442124</td>\n",
       "      <td>-0.621449</td>\n",
       "      <td>Day 24-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9431</th>\n",
       "      <td>-1.441780</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>2.492943</td>\n",
       "      <td>2.289345</td>\n",
       "      <td>Day 12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13078</th>\n",
       "      <td>-0.047084</td>\n",
       "      <td>0.543089</td>\n",
       "      <td>1.502882</td>\n",
       "      <td>1.567946</td>\n",
       "      <td>Day 18-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>0.652287</td>\n",
       "      <td>0.758303</td>\n",
       "      <td>0.681693</td>\n",
       "      <td>-0.214407</td>\n",
       "      <td>Day 06-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            d97       d98       d99      d100    samples\n",
       "5100   0.881895 -1.463558  0.309755 -0.156539  Day 06-09\n",
       "15125  1.099650 -1.794608 -1.442124 -0.621449  Day 24-27\n",
       "9431  -1.441780  0.068481  2.492943  2.289345  Day 12-15\n",
       "13078 -0.047084  0.543089  1.502882  1.567946  Day 18-21\n",
       "3193   0.652287  0.758303  0.681693 -0.214407  Day 06-09"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: False\n",
    "eb.train_ds.df[eb.train_ds.df.columns[-5:]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: False\n",
    "eb.train_ds.t_min, eb.train_ds.t_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 5, 100]), torch.Size([8, 5]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: False\n",
    "for x, y in dl:\n",
    "    break\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 100]), torch.Size([5]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: False\n",
    "for x, y in eb.train_ds:\n",
    "    break\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
